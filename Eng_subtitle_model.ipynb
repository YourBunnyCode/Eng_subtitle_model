{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача:\n",
    "Разработать ML решение для автоматического определения уровня сложности англоязычных фильмов.\n",
    "\n",
    "Цель:\n",
    "Получить метрику качества не ниже 0.6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import spacy\n",
    "from string import punctuation\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import srt\n",
    "from chardet import detect\n",
    "\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "RANDOM_STATE = 10\n",
    "dir_path = \"./Subtitles_all\"\n",
    "df_excel_path = \"./movies_labels.xlsx\"\n",
    "word2vec_path = \"./GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим файлы и проведем их перекодировку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subtitles = pd.DataFrame([], columns=[\"movie\", \"subs_raw\"])\n",
    "\n",
    "for root, dir, files in os.walk(dir_path):\n",
    "    for file_name in files:\n",
    "        if file_name.endswith(\".srt\"):\n",
    "            file_path = os.path.join(root, file_name)\n",
    "\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                data = f.read()\n",
    "                f_charInfo = detect(data)[\"encoding\"]\n",
    "                coding = str(f_charInfo)\n",
    "\n",
    "                with open(file_path, \"r\", encoding=coding) as sourceFile:\n",
    "                    contents = sourceFile.read()\n",
    "                    subtitle_generator = srt.parse(contents)\n",
    "                    subtitles = list(subtitle_generator)\n",
    "                    subtitles_content = map(lambda x: x.content, subtitles)\n",
    "\n",
    "                    subs = \" \".join(subtitles_content)\n",
    "                    movie = file_name[:-4]\n",
    "                    df_subs = pd.DataFrame({\"movie\": [movie], \"subs_raw\": [subs]})\n",
    "                    df_subtitles = pd.concat([df_subtitles, df_subs], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>subs_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crown, The S01E01 - Wolferton Splash.en</td>\n",
       "      <td>In seeking his British\\nnationalization, His R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Suits.Episode 1- Denial</td>\n",
       "      <td>You're the most amazing woman\\nI have ever met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...</td>\n",
       "      <td>(HARVEY READING) I've been after Sutter\\nfor t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suits.S02E08.HDTV.x264-EVOLVE</td>\n",
       "      <td>[Car horn blares] You're late. Nope. 30 second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Virgin.River.S01E07.INTERNAL.720p.WEB.x264-STRiFE</td>\n",
       "      <td>Are you sure I can't convince you to stay? No....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>Matilda(1996)</td>\n",
       "      <td>&lt;i&gt;&lt;font color=\"#808080\"&gt;(MUSIC)&lt;/font&gt;&lt;/i&gt; &lt;i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Her(2013)</td>\n",
       "      <td>Advertise your product or brand here\\ncontact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>The_Fundamentals_of_Caring(2016)</td>\n",
       "      <td>Caregiving is not just\\nabout feeding and clot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>The_Intern(2015)</td>\n",
       "      <td>Freud said, \"love and work.\\nWork and love. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>&lt;i&gt;Oh, I come from a land\\nFrom a faraway plac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 movie  \\\n",
       "0              Crown, The S01E01 - Wolferton Splash.en   \n",
       "1                              Suits.Episode 1- Denial   \n",
       "2    Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...   \n",
       "3                        Suits.S02E08.HDTV.x264-EVOLVE   \n",
       "4    Virgin.River.S01E07.INTERNAL.720p.WEB.x264-STRiFE   \n",
       "..                                                 ...   \n",
       "273                                      Matilda(1996)   \n",
       "274                                          Her(2013)   \n",
       "275                   The_Fundamentals_of_Caring(2016)   \n",
       "276                                   The_Intern(2015)   \n",
       "277                                      Aladdin(1992)   \n",
       "\n",
       "                                              subs_raw  \n",
       "0    In seeking his British\\nnationalization, His R...  \n",
       "1    You're the most amazing woman\\nI have ever met...  \n",
       "2    (HARVEY READING) I've been after Sutter\\nfor t...  \n",
       "3    [Car horn blares] You're late. Nope. 30 second...  \n",
       "4    Are you sure I can't convince you to stay? No....  \n",
       "..                                                 ...  \n",
       "273  <i><font color=\"#808080\">(MUSIC)</font></i> <i...  \n",
       "274  Advertise your product or brand here\\ncontact ...  \n",
       "275  Caregiving is not just\\nabout feeding and clot...  \n",
       "276  Freud said, \"love and work.\\nWork and love. Th...  \n",
       "277  <i>Oh, I come from a land\\nFrom a faraway plac...  \n",
       "\n",
       "[278 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subtitles"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем файл с данными о категориях фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_excel = pd.read_excel(df_excel_path)\n",
    "df_excel.columns = df_excel.columns.str.lower()\n",
    "df_excel.drop(columns=[\"id\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>Matilda(2022)</td>\n",
       "      <td>C1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>Bullet train</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>Thor: love and thunder</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Lightyear</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>The Grinch</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                movie   level\n",
       "0           10_Cloverfield_lane(2016)      B1\n",
       "1    10_things_I_hate_about_you(1999)      B1\n",
       "2                A_knights_tale(2001)      B2\n",
       "3                A_star_is_born(2018)      B2\n",
       "4                       Aladdin(1992)  A2/A2+\n",
       "..                                ...     ...\n",
       "236                     Matilda(2022)      C1\n",
       "237                      Bullet train      B1\n",
       "238            Thor: love and thunder      B2\n",
       "239                         Lightyear      B2\n",
       "240                        The Grinch      B1\n",
       "\n",
       "[241 rows x 2 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_excel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соединим обе таблицы в одну по столбцу movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suits.Episode 1- Denial</td>\n",
       "      <td>B2</td>\n",
       "      <td>You're the most amazing woman\\nI have ever met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...</td>\n",
       "      <td>B2</td>\n",
       "      <td>(HARVEY READING) I've been after Sutter\\nfor t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suits.S02E08.HDTV.x264-EVOLVE</td>\n",
       "      <td>B2</td>\n",
       "      <td>[Car horn blares] You're late. Nope. 30 second...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suits.S02E04.HDTV.x264-ASAP</td>\n",
       "      <td>B2</td>\n",
       "      <td>I want to, uh... taupe. - Is that...?\\n- Justi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suits.S02E09.HDTV.x264-ASAP</td>\n",
       "      <td>B2</td>\n",
       "      <td>Donna... Donna. You know, in all the years\\nth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Matilda(1996)</td>\n",
       "      <td>B1</td>\n",
       "      <td>&lt;i&gt;&lt;font color=\"#808080\"&gt;(MUSIC)&lt;/font&gt;&lt;/i&gt; &lt;i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Her(2013)</td>\n",
       "      <td>A2/A2+, B1</td>\n",
       "      <td>Advertise your product or brand here\\ncontact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>The_Fundamentals_of_Caring(2016)</td>\n",
       "      <td>B1</td>\n",
       "      <td>Caregiving is not just\\nabout feeding and clot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>The_Intern(2015)</td>\n",
       "      <td>B2</td>\n",
       "      <td>Freud said, \"love and work.\\nWork and love. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "      <td>&lt;i&gt;Oh, I come from a land\\nFrom a faraway plac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 movie       level  \\\n",
       "0                              Suits.Episode 1- Denial          B2   \n",
       "1    Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...          B2   \n",
       "2                        Suits.S02E08.HDTV.x264-EVOLVE          B2   \n",
       "3                          Suits.S02E04.HDTV.x264-ASAP          B2   \n",
       "4                          Suits.S02E09.HDTV.x264-ASAP          B2   \n",
       "..                                                 ...         ...   \n",
       "228                                      Matilda(1996)          B1   \n",
       "229                                          Her(2013)  A2/A2+, B1   \n",
       "230                   The_Fundamentals_of_Caring(2016)          B1   \n",
       "231                                   The_Intern(2015)          B2   \n",
       "232                                      Aladdin(1992)      A2/A2+   \n",
       "\n",
       "                                              subs_raw  \n",
       "0    You're the most amazing woman\\nI have ever met...  \n",
       "1    (HARVEY READING) I've been after Sutter\\nfor t...  \n",
       "2    [Car horn blares] You're late. Nope. 30 second...  \n",
       "3    I want to, uh... taupe. - Is that...?\\n- Justi...  \n",
       "4    Donna... Donna. You know, in all the years\\nth...  \n",
       "..                                                 ...  \n",
       "228  <i><font color=\"#808080\">(MUSIC)</font></i> <i...  \n",
       "229  Advertise your product or brand here\\ncontact ...  \n",
       "230  Caregiving is not just\\nabout feeding and clot...  \n",
       "231  Freud said, \"love and work.\\nWork and love. Th...  \n",
       "232  <i>Oh, I come from a land\\nFrom a faraway plac...  \n",
       "\n",
       "[233 rows x 3 columns]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = df_subtitles.merge(df_excel, how=\"inner\", on=[\"movie\"])\n",
    "data = data[[\"movie\", \"level\", \"subs_raw\"]]\n",
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем очистку текста от ненужных символов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_n = re.compile(\"\\n\")  # перенос каретки\n",
    "del_tags = re.compile(\"<[^>]*>\")  # html-теги (с содержимым)\n",
    "del_brackets = re.compile(\n",
    "    \"\\([^)]*\\)|\\[[^]]*\\]\"\n",
    ")  # содержимое круглых и квадратных скобок\n",
    "# clean_text = re.compile('[^а-яa-z\\'\\s]')          # все небуквенные символы кроме пробела и апострофа (')\n",
    "del_spaces = re.compile(\"\\s{2,}\")  # лишние символы пробелов/табуляции\n",
    "\n",
    "def prepare_text(text):\n",
    "    text = del_n.sub(\" \", str(text).lower())\n",
    "    text = del_tags.sub(\"\", text)\n",
    "    text = del_brackets.sub(\"\", text)\n",
    "    # text = clean_text.sub('', text)\n",
    "\n",
    "    return del_spaces.sub(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_std = data.copy()\n",
    "data_std[\"subs\"] = data_std[\"subs_raw\"].apply(lambda x: prepare_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_raw</th>\n",
       "      <th>subs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suits.Episode 1- Denial</td>\n",
       "      <td>B2</td>\n",
       "      <td>You're the most amazing woman\\nI have ever met...</td>\n",
       "      <td>you're the most amazing woman i have ever met....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...</td>\n",
       "      <td>B2</td>\n",
       "      <td>(HARVEY READING) I've been after Sutter\\nfor t...</td>\n",
       "      <td>i've been after sutter for three years now. t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suits.S02E08.HDTV.x264-EVOLVE</td>\n",
       "      <td>B2</td>\n",
       "      <td>[Car horn blares] You're late. Nope. 30 second...</td>\n",
       "      <td>you're late. nope. 30 seconds early. good. le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suits.S02E04.HDTV.x264-ASAP</td>\n",
       "      <td>B2</td>\n",
       "      <td>I want to, uh... taupe. - Is that...?\\n- Justi...</td>\n",
       "      <td>i want to, uh... taupe. - is that...? - justic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suits.S02E09.HDTV.x264-ASAP</td>\n",
       "      <td>B2</td>\n",
       "      <td>Donna... Donna. You know, in all the years\\nth...</td>\n",
       "      <td>donna... donna. you know, in all the years tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Matilda(1996)</td>\n",
       "      <td>B1</td>\n",
       "      <td>&lt;i&gt;&lt;font color=\"#808080\"&gt;(MUSIC)&lt;/font&gt;&lt;/i&gt; &lt;i...</td>\n",
       "      <td>narrator: everyone is born, but not everyone ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Her(2013)</td>\n",
       "      <td>A2/A2+, B1</td>\n",
       "      <td>Advertise your product or brand here\\ncontact ...</td>\n",
       "      <td>advertise your product or brand here contact w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>The_Fundamentals_of_Caring(2016)</td>\n",
       "      <td>B1</td>\n",
       "      <td>Caregiving is not just\\nabout feeding and clot...</td>\n",
       "      <td>caregiving is not just about feeding and cloth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>The_Intern(2015)</td>\n",
       "      <td>B2</td>\n",
       "      <td>Freud said, \"love and work.\\nWork and love. Th...</td>\n",
       "      <td>freud said, \"love and work. work and love. tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "      <td>&lt;i&gt;Oh, I come from a land\\nFrom a faraway plac...</td>\n",
       "      <td>oh, i come from a land from a faraway place wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 movie       level  \\\n",
       "0                              Suits.Episode 1- Denial          B2   \n",
       "1    Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...          B2   \n",
       "2                        Suits.S02E08.HDTV.x264-EVOLVE          B2   \n",
       "3                          Suits.S02E04.HDTV.x264-ASAP          B2   \n",
       "4                          Suits.S02E09.HDTV.x264-ASAP          B2   \n",
       "..                                                 ...         ...   \n",
       "228                                      Matilda(1996)          B1   \n",
       "229                                          Her(2013)  A2/A2+, B1   \n",
       "230                   The_Fundamentals_of_Caring(2016)          B1   \n",
       "231                                   The_Intern(2015)          B2   \n",
       "232                                      Aladdin(1992)      A2/A2+   \n",
       "\n",
       "                                              subs_raw  \\\n",
       "0    You're the most amazing woman\\nI have ever met...   \n",
       "1    (HARVEY READING) I've been after Sutter\\nfor t...   \n",
       "2    [Car horn blares] You're late. Nope. 30 second...   \n",
       "3    I want to, uh... taupe. - Is that...?\\n- Justi...   \n",
       "4    Donna... Donna. You know, in all the years\\nth...   \n",
       "..                                                 ...   \n",
       "228  <i><font color=\"#808080\">(MUSIC)</font></i> <i...   \n",
       "229  Advertise your product or brand here\\ncontact ...   \n",
       "230  Caregiving is not just\\nabout feeding and clot...   \n",
       "231  Freud said, \"love and work.\\nWork and love. Th...   \n",
       "232  <i>Oh, I come from a land\\nFrom a faraway plac...   \n",
       "\n",
       "                                                  subs  \n",
       "0    you're the most amazing woman i have ever met....  \n",
       "1     i've been after sutter for three years now. t...  \n",
       "2     you're late. nope. 30 seconds early. good. le...  \n",
       "3    i want to, uh... taupe. - is that...? - justic...  \n",
       "4    donna... donna. you know, in all the years tha...  \n",
       "..                                                 ...  \n",
       "228   narrator: everyone is born, but not everyone ...  \n",
       "229  advertise your product or brand here contact w...  \n",
       "230  caregiving is not just about feeding and cloth...  \n",
       "231  freud said, \"love and work. work and love. tha...  \n",
       "232  oh, i come from a land from a faraway place wh...  \n",
       "\n",
       "[233 rows x 4 columns]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_std"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "B2            97\n",
       "B1            53\n",
       "C1            39\n",
       "A2/A2+        25\n",
       "B1, B2         8\n",
       "A2             6\n",
       "A2/A2+, B1     5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_std.level.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединим малочисленные категории с основными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "level\n",
       "B2    97\n",
       "B1    61\n",
       "C1    39\n",
       "A2    36\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_std.loc[data_std[\"level\"] == \"A2/A2+\", \"level\"] = \"A2\"\n",
    "data_std.loc[data_std[\"level\"] == \"A2/A2+, B1\", \"level\"] = \"A2\"\n",
    "data_std.loc[data_std[\"level\"] == \"B1, B2\", \"level\"] = \"B1\"\n",
    "data_std.level.value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить модель с помощью Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиваем текст на токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie</th>\n",
       "      <th>level</th>\n",
       "      <th>subs_raw</th>\n",
       "      <th>subs</th>\n",
       "      <th>subs_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suits.Episode 1- Denial</td>\n",
       "      <td>B2</td>\n",
       "      <td>You're the most amazing woman\\nI have ever met...</td>\n",
       "      <td>you're the most amazing woman i have ever met....</td>\n",
       "      <td>[you, re, the, most, amazing, woman, i, have, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...</td>\n",
       "      <td>B2</td>\n",
       "      <td>(HARVEY READING) I've been after Sutter\\nfor t...</td>\n",
       "      <td>i've been after sutter for three years now. t...</td>\n",
       "      <td>[i, ve, been, after, sutter, for, three, years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Suits.S02E08.HDTV.x264-EVOLVE</td>\n",
       "      <td>B2</td>\n",
       "      <td>[Car horn blares] You're late. Nope. 30 second...</td>\n",
       "      <td>you're late. nope. 30 seconds early. good. le...</td>\n",
       "      <td>[you, re, late, nope, 30, seconds, early, good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Suits.S02E04.HDTV.x264-ASAP</td>\n",
       "      <td>B2</td>\n",
       "      <td>I want to, uh... taupe. - Is that...?\\n- Justi...</td>\n",
       "      <td>i want to, uh... taupe. - is that...? - justic...</td>\n",
       "      <td>[i, want, to, uh, taupe, is, that, justice, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Suits.S02E09.HDTV.x264-ASAP</td>\n",
       "      <td>B2</td>\n",
       "      <td>Donna... Donna. You know, in all the years\\nth...</td>\n",
       "      <td>donna... donna. you know, in all the years tha...</td>\n",
       "      <td>[donna, donna, you, know, in, all, the, years,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               movie level  \\\n",
       "0                            Suits.Episode 1- Denial    B2   \n",
       "1  Crazy4TV.com - Suits.S06E06.720p.BluRay.x265.H...    B2   \n",
       "2                      Suits.S02E08.HDTV.x264-EVOLVE    B2   \n",
       "3                        Suits.S02E04.HDTV.x264-ASAP    B2   \n",
       "4                        Suits.S02E09.HDTV.x264-ASAP    B2   \n",
       "\n",
       "                                            subs_raw  \\\n",
       "0  You're the most amazing woman\\nI have ever met...   \n",
       "1  (HARVEY READING) I've been after Sutter\\nfor t...   \n",
       "2  [Car horn blares] You're late. Nope. 30 second...   \n",
       "3  I want to, uh... taupe. - Is that...?\\n- Justi...   \n",
       "4  Donna... Donna. You know, in all the years\\nth...   \n",
       "\n",
       "                                                subs  \\\n",
       "0  you're the most amazing woman i have ever met....   \n",
       "1   i've been after sutter for three years now. t...   \n",
       "2   you're late. nope. 30 seconds early. good. le...   \n",
       "3  i want to, uh... taupe. - is that...? - justic...   \n",
       "4  donna... donna. you know, in all the years tha...   \n",
       "\n",
       "                                         subs_tokens  \n",
       "0  [you, re, the, most, amazing, woman, i, have, ...  \n",
       "1  [i, ve, been, after, sutter, for, three, years...  \n",
       "2  [you, re, late, nope, 30, seconds, early, good...  \n",
       "3  [i, want, to, uh, taupe, is, that, justice, th...  \n",
       "4  [donna, donna, you, know, in, all, the, years,...  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "data_std[\"subs_tokens\"] = data_std[\"subs\"].apply(tokenizer.tokenize)\n",
    "data_std.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем модель Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=300):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [\n",
    "            vector[word] if word in vector else np.random.rand(k)\n",
    "            for word in tokens_list\n",
    "        ]\n",
    "    else:\n",
    "        vectorized = [\n",
    "            vector[word] if word in vector else np.zeros(k) for word in tokens_list\n",
    "        ]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "\n",
    "def get_word2vec_embeddings(vectors, data_std, generate_missing=False):\n",
    "    embeddings = data_std[\"subs_tokens\"].apply(\n",
    "        lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing)\n",
    "    )\n",
    "    return list(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = get_word2vec_embeddings(word2vec, data_std)\n",
    "X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(\n",
    "    embeddings, data_std[\"level\"], test_size=0.2, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v = LogisticRegression()\n",
    "model_w2v.fit(X_train_word2vec, y_train_word2vec)\n",
    "y_predicted_word2vec = model_w2v.predict(X_test_word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48936170212765956"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = metrics.accuracy_score(y_test_word2vec, y_predicted_word2vec)\n",
    "accuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем обработать данные с помощью библиотеки SpaCy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_std[\"subs\"]\n",
    "ylabels = data_std[\"level\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, ylabels, test_size=0.2, random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим стоп-слова и пробелы между строками, проведем лемматизацию данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "def spacy_tokenizer(sentence):\n",
    "    mytokens = nlp(sentence)\n",
    "    mytokens = [\n",
    "        word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_\n",
    "        for word in mytokens\n",
    "    ]\n",
    "    mytokens = [\n",
    "        word for word in mytokens if word not in stop_words and word not in punctuation\n",
    "    ]\n",
    "\n",
    "    return mytokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим пайплайн для векторизации данных и модели LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = [\n",
    "    (\n",
    "        \"tfidf_vectorizer\",\n",
    "        TfidfVectorizer(tokenizer=spacy_tokenizer, token_pattern=None),\n",
    "    ),\n",
    "    (\n",
    "        \"classifier\",\n",
    "        LogisticRegression(),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(pipelines)\n",
    "pipe.fit(X_train, y_train)\n",
    "predicted = pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.6808510638297872\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Accuracy:\", metrics.accuracy_score(y_test, predicted))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В проекте рассматривались 2 подхода к обработке даных - с помощью библиотеки SpaCy и готовой модели библиотеки Gensim. Наилучший показатель дал первый подход. С его пмощью удалось добиться значения метрики - 0.68."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
